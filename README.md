# OPT4FL
Optimization algorithm of federation learning
## First-order Optimization
### FedAvg
* [2017_AIS] Communication-Efficient Learning of Deep Networks from Decentralized Data. In Artificial intelligence and statistics, [paper](https://proceedings.mlr.press/v54/mcmahan17a?ref=https://githubhelp.com).
* [2020_ICML] SCAFFOLD: Stochastic controlled averaging for federated learning. In International conference on machine learning, [paper](https://proceedings.mlr.press/v119/karimireddy20a.html).
* [2020_PMLS_fedprox] Federated optimization in heterogeneous networks. Proceedings of Machine learning and systems, [paper](https://proceedings.mlsys.org/paper_files/paper/2020/hash/1f5fe83998a09396ebe6477d9475ba0c-Abstract.html).
* [2019_AAAI] Parallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning, Proceedings of the AAAI conference on artificial intelligence, [paper](https://ojs.aaai.org/index.php/AAAI/article/view/4514).
* [2018_NIPS] LAG: Lazily aggregated gradient for communication-efficient distributed learning.Advances in neural information processing systems, [paper](https://proceedings.neurips.cc/paper/2018/hash/feecee9f1643651799ede2740927317a-Abstract.html).
* [2020_TWC] A joint learning and communications framework for federated learning over wireless networks.IEEE Transactions on Wireless Communications, [paper](https://ieeexplore.ieee.org/document/9210812).
* [2020_TWC] Federated learning over wireless fading channels.IEEE Transactions on Wireless Communications, [paper](https://ieeexplore.ieee.org/abstract/document/9014530).
### ADMM
* [2021_TSP]  Fedpd: A federated learning framework with adaptivity to non-iid data. IEEE Transactions on Signal Processing, [paper](https://ieeexplore.ieee.org/abstract/document/9556559).
 * [2022_ICDE]  FedADMM: A robust federated deep learning framework with adaptivity to system heterogeneity. IEEE Transactions on Signal Processing, [paper](https://ieeexplore.ieee.org/abstract/document/9835545).
 * [2021_TPAMI]  Federated learning via inexact ADMM. IEEE Transactions on Signal Processing, [paper](https://ieeexplore.ieee.org/abstract/document/10040221).
 * [2019_TIFS]  DP-ADMM: ADMM-based distributed learning with differential privac. IEEE Transactions on Information Forensics and Security, [paper](https://ieeexplore.ieee.org/abstract/document/8772211).
* [2018]  Recycled ADMM: Improve privacy and accuracy with less computation in distributed algorithmsa. 2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton), [paper](https://ieeexplore.ieee.org/abstract/document/8635916).
## Second-order Optimization
### Newtonâ€™s method
* [2020_NIPS20] Distributed Newton Can Communicate Less and Resist Byzantine Workers. proceedings of the 34th International Conference on Neural Information Processing Systems, [paper](https://proceedings.neurips.cc/paper/2020/file/d17e6bcbcef8de3f7a00195cfa5706f1-Paper.pdf).
* [2018_NIPS18]GIANT: Globally improved approximate newton method for distributed optimization. Advances in neural information processing systems, [paper](https://proceedings.neurips.cc/paper/2018/hash/dabd8d2ce74e782c65a973ef76fd540b-Abstract.html).
* [2021] LocalNewton: Reducing Communication Bottleneck for Distributed Learning. arxiv, [paper](https://arxiv.org/abs/2105.07320).
* [2021_ICML] Communication-efficient distributed optimization with quantized preconditioners. International conference on machine learning, [paper](https://proceedings.mlr.press/v139/alimisis21a.html).
* [2022_ICML] FedNew: A Communication-Efficient and Privacy-Preserving Newton-Type Method for Federated Learning. In International conference on machine learning,[paper](https://proceedings.mlr.press/v162/elgabli22a/elgabli22a.pdf).
* [2022_ICML] FedNL: Making Newton-Type Methods Applicable to Federated Learning. In International conference on machine learning,[paper](https://proceedings.mlr.press/v162/safaryan22a.html).
* [2022] Flecs: A federated learning second-order framework via compression and sketching. arxiv, [paper](https://arxiv.org/abs/2206.02009).
* [2022_TNSE] Resource-constrained federated learning with heterogeneous data: Formulation and analysis. IEEE Transactions on Network Science and Engineering, [paper](https://ieeexplore.ieee.org/document/9609654).
* [2022_AIS] Basis Matters: Better Communication-Efficient Second Order Methods for Federated Learning. Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, [paper](https://proceedings.mlr.press/v151/qian22a.html).
* [2022_TPDS] Done: Distributed approximate Newton-type method for federated edge learning. IEEE Transactions on Parallel and Distributed Systems, [paper](https://ieeexplore.ieee.org/abstract/document/9695269).
* [2024_AUTO]Shed: A Newton-type algorithm for federated learning based on incremental hessian eigenvector sharing. Automatica,[paper](https://www.sciencedirect.com/science/article/pii/S0005109823006271).
* [2024_AAAI] Fedns: A fast sketching newton-type algorithm for federated learning. In AAAI,[paper](https://ojs.aaai.org/index.php/AAAI/article/view/29254).
* [2024] GP-FL: Model-Based Hessian Estimation for Second-Order Over-the-Air Federated Learning. arxiv, [paper](https://arxiv.org/abs/2412.03867).

### Quasi-Newton Method
* [2025] Distributed Quasi-Newton Method for Fair and Fast Federated Learning. arxiv, [paper](https://arxiv.org/abs/2501.10877),[code](https://anonymous.4open.science/r/DQN-Fed-FDD2/README.md)
* 
