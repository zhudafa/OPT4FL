# OPT4FL
Optimization algorithm of federation learning
## First-order Optimization
### FedAvg
* [2017_AIS] Communication-Efficient Learning of Deep Networks from Decentralized Data. In Artificial intelligence and statistics, [paper](https://proceedings.mlr.press/v54/mcmahan17a?ref=https://githubhelp.com).
* [2020_ICML] SCAFFOLD: Stochastic controlled averaging for federated learning. In International conference on machine learning, [paper](https://proceedings.mlr.press/v119/karimireddy20a.html).
* [2020_PMLS_fedprox] Federated optimization in heterogeneous networks. Proceedings of Machine learning and systems, [paper](https://proceedings.mlsys.org/paper_files/paper/2020/hash/1f5fe83998a09396ebe6477d9475ba0c-Abstract.html).
* [2019_AAAI] Parallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning, Proceedings of the AAAI conference on artificial intelligence, [paper](https://ojs.aaai.org/index.php/AAAI/article/view/4514).
* [2018_NIPS] LAG: Lazily aggregated gradient for communication-efficient distributed learning.Advances in neural information processing systems, [paper](https://proceedings.neurips.cc/paper/2018/hash/feecee9f1643651799ede2740927317a-Abstract.html).
* [2020_TWC] A joint learning and communications framework for federated learning over wireless networks.IEEE Transactions on Wireless Communications, [paper](https://ieeexplore.ieee.org/document/9210812).
* [2020_TWC] Federated learning over wireless fading channels.IEEE Transactions on Wireless Communications, [paper](https://ieeexplore.ieee.org/abstract/document/9014530).
## Second-order Optimization
### Newtonâ€™s method
* [2020_NIPS20] Distributed Newton Can Communicate Less and Resist Byzantine Workers. proceedings of the 34th International Conference on Neural Information Processing Systems, [paper](https://proceedings.neurips.cc/paper/2020/file/d17e6bcbcef8de3f7a00195cfa5706f1-Paper.pdf).
* [2021] LocalNewton: Reducing Communication Bottleneck for Distributed Learning. arxiv, [paper](https://arxiv.org/abs/2105.07320).
* [2022_ICML]FedNew: A Communication-Efficient and Privacy-Preserving Newton-Type Method for Federated Learning. In International conference on machine learning,[paper](https://proceedings.mlr.press/v162/elgabli22a/elgabli22a.pdf).
* [2022_ICML]FedNL: Making Newton-Type Methods Applicable to Federated Learning. In International conference on machine learning,[paper](https://proceedings.mlr.press/v162/safaryan22a.html).
* [2024_AUTO]Shed: A newton-type algorithm for federated learning based on incremental hessian eigenvector sharing. Automatica,[paper](https://www.sciencedirect.com/science/article/pii/S0005109823006271).
* [2024_AAAI] Fedns: A fast sketching newton-type algorithm for federated learning. In AAAI,[paper](https://ojs.aaai.org/index.php/AAAI/article/view/29254).
* [2024] GP-FL: Model-Based Hessian Estimation for Second-Order Over-the-Air Federated Learning. arxiv, [paper](https://arxiv.org/abs/2412.03867).
### Quasi-Newton Method
* [2025] Distributed Quasi-Newton Method for Fair and Fast Federated Learning. arxiv, [paper](https://arxiv.org/abs/2501.10877),[code](https://anonymous.4open.science/r/DQN-Fed-FDD2/README.md)
